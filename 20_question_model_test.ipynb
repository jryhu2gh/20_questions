{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f5381e-5046-408d-8468-c3fe09692204",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db98908c-3a28-43c7-93a5-ce9c605ea482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is used\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CFG:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 2\n",
    "    DROPOUT = 0.05\n",
    "    # MODEL_NAME = \"../llama3/Meta-Llama-3-8B/\"\n",
    "    MODEL_NAME = \"C:/Users/jryhu/Documents/Git/llama-3-transformers-8b-chat-hf-v1/\"\n",
    "    SEED = 2024\n",
    "    MAX_LENGTH = 128 # truncate the input to save memory, toy implementation only\n",
    "    NUM_WARMUP_STEPS = 4 # toy implementation. \n",
    "    LR_MAX = 5E-5\n",
    "    NUM_CLASS_LLAMA = 128 # with the llama model, we hope it can generate 128 features, which are combined with tfidf features.\n",
    "    NUM_LABELS = 3 # The final number of labels\n",
    "    LORA_RANK = 1 # Toy implementation \n",
    "    LORA_ALPHA = 2 # toy implementation\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('GPU is used')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('CPU is used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e77de9a7-0a86-4bf3-a3bf-f78e8bb67d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d136acbc21ab4f5db1087b7705852794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# AGENT_PATH = \"./agent/\"\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(CFG.MODEL_NAME, torch_dtype = torch.bfloat16, device_map = \"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea7ddb2-52cc-43bb-acca-8e9a5d353001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "863207b7-f836-47bf-8838-b511d083986e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class asker():\n",
    "    def __init__(self):\n",
    "        sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "                the user is going to think of a word, it can be only one of the following 3 categories:\n",
    "                1. a place\n",
    "                2. a person\n",
    "                3. a thing\n",
    "                So focus your area of search on these options. and give smart questions that narrows down the search space\\n\"\"\"\n",
    "        ask_prompt = sys_prompt + \"\"\"your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
    "                    to help you, here's an example of how it should work assuming that the keyword is Morocco:\n",
    "                    examle:\n",
    "                    <you: is it a place?\n",
    "                    user: yes\n",
    "                    you: is it in europe?\n",
    "                    user: no\n",
    "                    you: is it in africa?\n",
    "                    user: yes\n",
    "                    you: do most people living there have dark skin?\n",
    "                    user: no\n",
    "                    user: is it a country name starting by m ?\n",
    "                    you: yes\n",
    "                    you: is it Morocco?\n",
    "                    user: yes.>\n",
    "\n",
    "                    the user has chosen the word, ask your first question!\n",
    "                    please be short and not verbose, give only one question, no extra word!\\n\\n\"\"\"\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\\n\\n\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        self.chat_template_root = chat_template\n",
    "        self.chat_template = chat_template\n",
    "    def append_history(self, question = None, answer = None, n=5):\n",
    "        chat_template = self.chat_template_root\n",
    "        ### only keep the most recent 5 questions\n",
    "        if len(question)>n:\n",
    "            for i in range(len(question)-n,len(question)):\n",
    "                chat_template += f\"{question[i]}<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{answer[i]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        else:\n",
    "            for q,a in zip(question, answer):\n",
    "                chat_template += f\"{q}<|eot_id|>\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        self.chat_template = chat_template\n",
    "        \n",
    "class answerer():\n",
    "    def __init__(self):\n",
    "        self.sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "                you are going to think of a word, it can be only one of the following 3 categories:\n",
    "                1. a place\n",
    "                2. a person\n",
    "                3. a thing\n",
    "                Based on this word, you will respond \"Yes\", \"No\", or \"I don't know\" to user's question. Remember, you can only respond \"Yes\", \"No\", or \"I don't know\".\\n\"\"\"        \n",
    "        \n",
    "#         chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{create_word_prompt}<|eot_id|>\"\"\"\n",
    "#         chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        self.chat_template = None # chat_template\n",
    "    def Create_Answer(self):\n",
    "        self.create_word_prompt = self.sys_prompt + \"\"\"Now, as a first step, please generate a single random word as the answer this this game. It can be a place, a person, or a thing. \n",
    "                    example 1:\n",
    "                    <Cat.> \\n\n",
    "                    example 2:\n",
    "                    <Bill Gates.> \\n\n",
    "                    \n",
    "                    Tell me only one word. No extra word!\\n\"\"\"\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{self.create_word_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        self.chat_template = chat_template\n",
    "        \n",
    "    def Answer(self, question = None, answer = None):\n",
    "        self.answer_prompt = self.sys_prompt + f\"\"\"\n",
    "                    Here is an example of how it should work assume that the answer is Morocco:\n",
    "                    examle:\n",
    "                    <user: is it a place?\n",
    "                    you: yes\n",
    "                    user: is it in europe?\n",
    "                    you: no\n",
    "                    user: is it in africa?\n",
    "                    you: yes\n",
    "                    user: do most people living there have dark skin?\n",
    "                    you: no\n",
    "                    user: is it a country name starting by m ?\n",
    "                    you: yes\n",
    "                    user: is it Morocco?\n",
    "                    yes: yes.>\n",
    "\n",
    "                    Your answer is {answer}. User's question is {question}.\n",
    "                    Now give me your respond. You can only respond \"Yes\", \"No\", or \"I don't know\"! No other words!\"\"\"\n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{self.answer_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        self.chat_template = chat_template\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5061d8e7-27c1-4a79-9704-bdd3f5f9c154",
   "metadata": {},
   "source": [
    "## Answer Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "508d2284-b2c7-4c76-b694-946015cd50fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ans = answerer()\n",
    "ans.Create_Answer()\n",
    "answer_word = generate_answer(ans.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e09e9c38-03dd-45bd-96e2-f73963a94811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "print(answer_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b3979b6-f9f7-4df4-91ac-c26d8ae6c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ans.Answer(question = \"Is this in Spain?\", answer = answer_word)\n",
    "out = generate_answer(ans.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91cc9014-848a-4212-b98d-cb683a7b4103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0b713-60eb-4cab-80dc-7231492db451",
   "metadata": {},
   "source": [
    "## Asker Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f29e805-5e8f-4e51-8002-a843f78fc28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "ask = asker()\n",
    "question = generate_answer(ask.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76d71830-d975-4909-9abb-3526b98d4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it a place?\n"
     ]
    }
   ],
   "source": [
    "print(question)\n",
    "question_list = []\n",
    "answer_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fd93db9-91b3-47d5-b6d3-022a92acfd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list.append(question)\n",
    "answer_list.append(\"Yes\")\n",
    "ask.append_history(question = question_list, answer=answer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7596e982-9a18-46be-b7df-d573784a5477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
      "                the user is going to think of a word, it can be only one of the following 3 categories:\n",
      "                1. a place\n",
      "                2. a person\n",
      "                3. a thing\n",
      "                So focus your area of search on these options. and give smart questions that narrows down the search space\n",
      "your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
      "                    to help you, here's an example of how it should work assuming that the keyword is Morocco:\n",
      "                    examle:\n",
      "                    <you: is it a place?\n",
      "                    user: yes\n",
      "                    you: is it in europe?\n",
      "                    user: no\n",
      "                    you: is it in africa?\n",
      "                    user: yes\n",
      "                    you: do most people living there have dark skin?\n",
      "                    user: no\n",
      "                    user: is it a country name starting by m ?\n",
      "                    you: yes\n",
      "                    you: is it Morocco?\n",
      "                    user: yes.>\n",
      "\n",
      "                    the user has chosen the word, ask your first question!\n",
      "                    please be short and not verbose, give only one question, no extra word!\n",
      "\n",
      "<|eot_id|>\n",
      "\n",
      "<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Is it a place?<|eot_id|>\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Yes<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ask.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7e011-dc99-40d9-9f52-f65baf508d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
