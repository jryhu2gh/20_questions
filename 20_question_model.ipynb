{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69241e78-6996-4530-a34c-4110bd79bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22b0ff9-8d98-4a65-be10-844c465332e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is used\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CFG:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 2\n",
    "    DROPOUT = 0.05\n",
    "    MODEL_NAME = \"../llama3/Meta-Llama-3-8B/\"\n",
    "    SEED = 2024\n",
    "    MAX_LENGTH = 128 # truncate the input to save memory, toy implementation only\n",
    "    NUM_WARMUP_STEPS = 4 # toy implementation. \n",
    "    LR_MAX = 5E-5\n",
    "    NUM_CLASS_LLAMA = 128 # with the llama model, we hope it can generate 128 features, which are combined with tfidf features.\n",
    "    NUM_LABELS = 3 # The final number of labels\n",
    "    LORA_RANK = 1 # Toy implementation \n",
    "    LORA_ALPHA = 2 # toy implementation\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda'\n",
    "    print('GPU is used')\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    print('CPU is used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39fa7f8b-d81b-4241-acf1-848b3c03473e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec491fb7db7344c5afb87ba358602839",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "AGENT_PATH = \"./agent/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(CFG.MODEL_NAME, torch_dtype = torch.bfloat16, device_map = \"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db7b5a3-3965-4a58-8171-b13a1fa0892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors = \"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids, max_new_tokesn=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop=out_ids.tolist().idex(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b365fbd-db15-4e42-8c57-e083c5980ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robot():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self,mode,obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            otuput = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            output = self.asker(obs)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def asker(self,obs):\n",
    "        sys_prompt = \"\"\"You are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "        the user is going to think of a word, it can be only one of the following 3 categories:\n",
    "        1. a place\n",
    "        2. a person\n",
    "        3. a thing\n",
    "        So focus your area of search on these options. and give smart questions that narrows down the search space\\n\"\"\"\n",
    "\n",
    "        if obs.turnType == \"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"your role is to find the word by asking him up to 20 questions, your questions to be valid must have only a 'yes' or 'no' answer.\n",
    "            to help you, here's an example of how it should work assuming that the keyword is Morocco:\n",
    "            examle:\n",
    "            <you: is it a place?\n",
    "            user: yes\n",
    "            you: is it in europe?\n",
    "            user: no\n",
    "            you: is it in africa?\n",
    "            user: yes\n",
    "            you: do most people living there have dark skin?\n",
    "            user: no\n",
    "            user: is it a country name starting by m ?\n",
    "            you: yes\n",
    "            you: is it Morocco?\n",
    "            user: yes.>\n",
    "\n",
    "            the user has chosen the word, ask your first question!\n",
    "            please be short and not verbose, give only one question, no extra word!\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answer):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answer):\n",
    "                conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "            guess_prompt = sys_prompt + f\"\"\"so far, the current state of the game is as following:\\n{conv}\n",
    "            based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "    def answerer(self,obs):\n",
    "        sys_prompt = f\"\"\"you are a helpful AI assistant, and your are very smart in playing 20 questions game,\n",
    "        the role of the user is to guess the word by asking you up to 20 questions, your answers to be valid must be a 'yes' or 'no', any other answer is invalid and you lose the game.\n",
    "        Know that the user will always guess a word belonging to one of the following 3 categories:\n",
    "        1. a place\n",
    "        2. a person\n",
    "        3. a thing\n",
    "        so make sure you understand the user's question and you understand the keyword you're playig on.\n",
    "        for now the word that the user should guess is: \"{obs.keyword}\", it is of category \"{obs.category}\",\n",
    "        to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n",
    "        examle:\n",
    "        <user: is it a place?\n",
    "        you: yes\n",
    "        user: is it in europe?\n",
    "        you: no\n",
    "        user: is it in africa?\n",
    "        you: yes\n",
    "        user: do most people living there have dark skin?\n",
    "        you: no\n",
    "        user: is it a country name starting by m ?\n",
    "        you: yes\n",
    "        user: is it Morocco?\n",
    "        you: yes.>\"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27a6c740-d130-43c9-9b19-522ecf64eb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4d9f722-43a7-442a-ab89-4706ec7f8bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class interation():\n",
    "    turnType = 'ask'\n",
    "    questions = [\"Is this alive?\"]\n",
    "    keyword = \"Cat\"\n",
    "    category = \"animal\"\n",
    "    answers = [\"cat\"]\n",
    "obs = interation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38ceb071-5b3e-40a1-9a98-acc60a9ca85f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['max_new_tokesn'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrobot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 10\u001b[0m, in \u001b[0;36mRobot.on\u001b[1;34m(self, mode, obs)\u001b[0m\n\u001b[0;32m      8\u001b[0m     otuput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masker(obs)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswering\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manswerer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mlower():\n\u001b[0;32m     12\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[32], line 101\u001b[0m, in \u001b[0;36mRobot.answerer\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         chat_template \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|eot_id|><|start_header_id|>user<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         chat_template \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m<|eot_id|><|start_header_id|>assistant<|end_header_id|>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 101\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_template\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[1;34m(template)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(template):\n\u001b[0;32m      2\u001b[0m     inp_ids \u001b[38;5;241m=\u001b[39m tokenizer(template, return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     out_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minp_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokesn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m      4\u001b[0m     start_gen \u001b[38;5;241m=\u001b[39m inp_ids\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m     out_ids \u001b[38;5;241m=\u001b[39m out_ids[start_gen:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1640\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1638\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[0;32m   1639\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(generation_config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1640\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_model_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1641\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_assistant(assistant_model)\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;66;03m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1238\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[1;34m(self, model_kwargs)\u001b[0m\n\u001b[0;32m   1235\u001b[0m         unused_model_args\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unused_model_args:\n\u001b[1;32m-> 1238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munused_model_args\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (note: typos in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m generate arguments will also show up in this list)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1241\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['max_new_tokesn'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "robot.on(\"answering\",obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993533a8-c618-4996-82d2-20000c340e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
